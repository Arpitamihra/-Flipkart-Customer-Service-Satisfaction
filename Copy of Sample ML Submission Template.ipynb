{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1756738219899}],"collapsed_sections":["yiiVWRdJDDil","-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n","##### **Contribution**    - Individual/Team\n","##### **Team Member 1 -**\n","##### **Team Member 2 -**\n","##### **Team Member 3 -**\n","##### **Team Member 4 -**"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["Project Name - Flipkart Customer Service Satisfaction\n","\n","Project Type - Classification (primary project) & EDA + Unsupervised (clustering for insights) → strengthens project scope.\n","\n","Contribution - Individual"],"metadata":{"id":"mCNq7acxYqu3"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["Write the summary here within 500-600 words."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["In today’s competitive e-commerce world, companies like Flipkart need to do much more than just sell products. Customers now expect quick responses, helpful service, and smooth problem resolution whenever they face any issues. If customers do not get proper support, they may stop using the platform and move to a competitor. This is why customer satisfaction has become one of the most important areas for Flipkart to focus on.\n","\n","This project is based on predicting and analyzing customer satisfaction using past service interaction data. The main idea is to study customer feedback, complaints, response times, and ratings, and then use Machine Learning techniques to classify whether a customer is satisfied, neutral, or dissatisfied with the service. Such insights can help Flipkart improve its customer support system, train its agents better, and take preventive actions before customers lose trust.\n","\n","Business Relevance\n","\n","Customer satisfaction is directly linked with customer loyalty and long-term business growth. For Flipkart, delivering excellent customer service not only helps in solving problems faster but also builds trust among millions of customers. By predicting customer satisfaction levels, Flipkart can:\n","\n","Identify key factors that cause dissatisfaction, such as late deliveries, payment issues, or poor agent behavior.\n","\n","Reward and train service agents based on their performance.\n","\n","Personalize solutions for different customer needs.\n","\n","Improve the overall Customer Satisfaction (CSAT) score, which is a key performance metric.\n","\n","Dataset\n","The dataset for this project contains records of customer support interactions. Each record represents a customer’s complaint or service request. Some of the key columns are:\n","\n","Channel: How the customer contacted support (Chat, Email, Phone, App).\n","\n","Complaint Text: The actual message or complaint from the customer.\n","\n","Category: The type of issue (Delivery, Refund, Payment, Product Quality, etc.).\n","\n","Response Time: The number of hours taken by the support team to reply.\n","\n","Resolution Status: Whether the issue was resolved or not.\n","\n","Customer Rating: The score (1–5) given by the customer after service.\n","\n","Satisfaction Level: The target variable, which shows if the customer was satisfied, neutral, or dissatisfied.\n","\n","If real Flipkart data is not available, similar e-commerce customer service datasets or synthetic data generated with tools like Faker can be used.\n","Data Exploration & Cleaning (EDA)\n","\n","Understand the dataset, remove missing values, and clean text data.\n","\n","Visualize important trends, such as common complaint categories or response time distribution.\n","\n","Feature Engineering\n","\n","Convert text complaints into meaningful features using methods like TF-IDF or sentiment analysis.\n","\n","Use numerical features like response time and resolution status.\n","\n","Model Building (Classification)\n","\n","Apply classification algorithms like Logistic Regression, Random Forest, XGBoost, and Support Vector Machines.\n","\n","Compare their performance using metrics like Accuracy, F1-score, and ROC-AUC.\n","\n","The best-performing model is selected to predict customer satisfaction.\n","\n","GenAI Integration (Optional but powerful)\n","\n","Use Azure OpenAI to summarize complaints and provide smart recommendations for agents.\n","\n","Example: “This customer is unhappy with late delivery. Suggest offering faster shipping next time.”\n","\n","Deployment\n","\n","Deploy the best model using Microsoft Azure Machine Learning.\n","\n","Make it available as a REST API that can be connected to other Flipkart systems.\n","\n","Use Power BI or Streamlit dashboards to visualize predictions and trends.\n","Accurately classify whether a customer is satisfied, neutral, or dissatisfied.\n","\n","Understand the main reasons behind customer dissatisfaction.\n","\n","Improve training and performance of support agents.\n","\n","Build dashboards that track customer sentiment in real time.\n","\n","This project does not just stop at predictions. It also provides actionable insights that Flipkart can use to improve service quality. With Machine Learning and Generative AI on Azure, customer feedback can be analyzed at scale, giving Flipkart a smarter way to keep customers happy and loyal."],"metadata":{"id":"bK6bcLMJfTjp"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**Write Problem Statement Here.**\n","In the e-commerce industry, customer satisfaction plays a vital role in ensuring long-term success and customer loyalty. Flipkart, being one of the largest online marketplaces in India, handles millions of customer service interactions every day. These interactions include queries, complaints, and feedback received through different support channels such as chat, email, phone, and app support.\n","\n","However, due to the large volume of cases, it becomes challenging to manually analyze and understand customer sentiments and satisfaction levels. Delays in responses, unresolved complaints, and negative service experiences often result in dissatisfied customers, which directly impacts the company’s reputation, Customer Satisfaction (CSAT) scores, and overall sales.\n","\n","The problem is to develop a machine learning classification model that can automatically predict customer satisfaction levels (Satisfied, Neutral, Dissatisfied) based on historical data such as complaint text, response time, resolution status, and customer ratings. By identifying dissatisfaction patterns early, Flipkart can take corrective actions, optimize service team performance, and provide better customer experiences.\n","\n","This solution will not only improve customer retention and trust but also support Flipkart’s goal of delivering excellent customer service in a highly competitive e-commerce market."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["!pip install faker"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# 📌 Flipkart CSAT Dataset Loading (Error-Free & Simple)\n","\n","import pandas as pd\n","import numpy as np\n","\n","# -----------------------------\n","# Generate synthetic dataset\n","# -----------------------------\n","np.random.seed(42)\n","\n","channels = [\"Chat\", \"Email\", \"Phone\", \"App\"]\n","categories = [\"Delivery\", \"Refund\", \"Payment\", \"Product Quality\", \"Technical\"]\n","\n","n_samples = 2000  # You can adjust this number\n","\n","data = {\n","    \"Customer_ID\": [f\"CUST{1000+i}\" for i in range(n_samples)],\n","    \"Channel\": np.random.choice(channels, n_samples),\n","    \"Category\": np.random.choice(categories, n_samples),\n","    \"Response_Time\": np.random.randint(1, 72, n_samples),\n","    \"Resolution_Status\": np.random.choice([\"Yes\", \"No\"], n_samples, p=[0.8, 0.2]),\n","    \"Customer_Rating\": np.random.randint(1, 6, n_samples)\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Map ratings → satisfaction level\n","df[\"Satisfaction_Level\"] = df[\"Customer_Rating\"].map(\n","    lambda r: \"Dissatisfied\" if r <= 2 else (\"Neutral\" if r == 3 else \"Satisfied\")\n",")\n","\n","# -----------------------------\n","# Dataset Preview\n","# -----------------------------\n","print(\"✅ Dataset Loaded Successfully!\")\n","print(df.head())\n","print(\"\\nShape of Dataset:\", df.shape)\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# 📌 Dataset First View - Flipkart CSAT\n","\n","# First 5 rows\n","print(\"🔹 First 5 Records:\")\n","display(df.head())\n","\n","# Last 5 rows\n","print(\"\\n🔹 Last 5 Records:\")\n","display(df.tail())\n","\n","# Random sample (5 rows)\n","print(\"\\n🔹 Random Sample of Records:\")\n","display(df.sample(5, random_state=42))\n"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# 📌 Dataset Rows & Columns Count - Flipkart CSAT\n","\n","rows, cols = df.shape\n","print(f\"✅ The dataset contains:\")\n","print(f\"   🔹 Number of Rows (Records): {rows}\")\n","print(f\"   🔹 Number of Columns (Features): {cols}\")"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","print(\"=== Dataset Info ===\")\n","df.info()\n","\n","print(\"\\n=== Summary Statistics (Numerical Columns) ===\")\n","print(df.describe())\n","\n","print(\"\\n=== Summary Statistics (Categorical Columns) ===\")\n","print(df.describe(include=['object']))\n","\n"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","# 📌 Check Duplicate Values - Flipkart CSAT\n","\n","# Count total duplicate rows\n","duplicate_count = df.duplicated().sum()\n","print(f\"Total Duplicate Rows: {duplicate_count}\")\n","\n","# Show sample duplicate records (if any)\n","if duplicate_count > 0:\n","    print(\"\\n🔎 Duplicate Records Found:\")\n","    display(df[df.duplicated()].head())\n","else:\n","    print(\"\\n✅ No Duplicate Records Found in Dataset.\")\n"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","# 📌 Missing / Null Values Check - Flipkart CSAT\n","\n","# Count missing values per column\n","missing_values = df.isnull().sum()\n","\n","print(\"=== Missing Values in Each Column ===\")\n","print(missing_values)\n","\n","# Total missing values in dataset\n","print(f\"\\n🔹 Total Missing Values: {missing_values.sum()}\")\n"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Heatmap for missing values\n","plt.figure(figsize=(8, 5))\n","sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n","plt.title(\"Missing Values Heatmap\", fontsize=14)\n","plt.show()\n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Answer Here\n","The dataset has 2000 records (rows) and 7 features (columns).\n","\n","It contains details of Flipkart customer service interactions like:\n","\n","Service channel (Chat, Email, Phone, App)\n","\n","Type of complaint category (Delivery, Refund, Payment, etc.)\n","\n","Response time taken to resolve (in hours)\n","\n","Resolution status (Yes/No)\n","\n","Customer rating (1–5)\n","\n","Satisfaction level (Satisfied, Neutral, Dissatisfied)\n","\n","There are no missing values and no duplicates.\n","\n","Dataset is a mix of categorical and numerical features."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","# 📌 Dataset Columns - Flipkart CSAT\n","\n","print(\"=== Dataset Columns ===\")\n","print(df.columns.tolist())\n"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","# 📌 Dataset Describe - Flipkart CSAT\n","\n","print(\"=== Numerical Features Summary ===\")\n","print(df.describe())\n","\n","print(\"\\n=== Categorical Features Summary ===\")\n","print(df.describe(include=['object']))\n"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Answer Here\n","Customer_ID → Unique code given to each customer.\n","\n","Channel → The way customer contacted support (Chat, Email, Phone, App).\n","\n","Category → Type of issue raised (Delivery, Refund, Payment, Product Quality, Technical).\n","\n","Response_Time → Time (in hours) taken to respond to the complaint.\n","\n","Resolution_Status → Shows if the issue was solved (Yes) or not (No).\n","\n","Customer_Rating → Rating given by customer (1 to 5 scale).\n","\n","Satisfaction_Level → Overall satisfaction: Satisfied (4–5), Neutral (3), Dissatisfied (1–2)."],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","\n","for col in df.columns:\n","    unique_vals = df[col].nunique()\n","    print(f\"{col}: {unique_vals} unique values\")\n","    print(df[col].unique()[:10], \"...\")  # show first 10 unique values as preview\n","    print(\"-\" * 50)\n"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code\n","\n","*   List item\n","*   List item\n","\n"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# -------------------------------\n","# 📌 Data Wrangling - Flipkart CSAT Dataset (Short Version)\n","# -------------------------------\n","\n","# 1. Remove duplicates\n","df = df.drop_duplicates().reset_index(drop=True)\n","\n","# 2. Handle missing values\n","print(\"Missing Values Before Cleaning:\\n\", df.isnull().sum())\n","# Example: fill numeric with median, categorical with mode\n","df[\"Response_Time\"] = df[\"Response_Time\"].fillna(df[\"Response_Time\"].median())\n","df[\"Category\"] = df[\"Category\"].fillna(df[\"Category\"].mode()[0])\n","\n","# 3. Convert IDs to categorical (only columns present)\n","id_cols = [col for col in [\"Customer_ID\", \"Agent_ID\"] if col in df.columns]\n","for col in id_cols:\n","    df[col] = df[col].astype(\"category\")\n","\n","# 4. Encode categorical columns\n","from sklearn.preprocessing import LabelEncoder\n","categorical_cols = [\"Channel\", \"Category\", \"Resolution_Status\", \"Satisfaction_Level\"]\n","\n","for col in categorical_cols:\n","    if col in df.columns:  # avoid KeyError\n","        le = LabelEncoder()\n","        df[col + \"_Encoded\"] = le.fit_transform(df[col])\n","\n","# 5. Final check\n","print(\"\\nMissing Values After Cleaning:\\n\", df.isnull().sum())\n","print(\"✅ Dataset is Analysis Ready! Shape:\", df.shape)\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Answer Here.\n","Manipulations Done\n","\n","Removed duplicate records.\n","\n","Checked & handled missing values (filled with median/mode if required).\n","\n","Converted ID columns into categorical type.\n","\n","Encoded text/categorical columns into numbers for ML use.\n","\n"," Insights Found\n","\n","Dataset is clean and ready for analysis.\n","\n","No major missing values or duplicate issues after cleaning.\n","\n","Customer issues are grouped into clear categories (Delivery, Refund, etc.).\n","\n","Satisfaction levels can be predicted using service features like response time, channel, and resolution status."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# -------------------------------\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(6,4))\n","sns.countplot(x=\"Satisfaction_Level\", data=df, palette=\"viridis\")\n","\n","plt.title(\"Distribution of Customer Satisfaction Levels\", fontsize=14)\n","plt.xlabel(\"Satisfaction Level\")\n","plt.ylabel(\"Number of Customers\")\n","plt.show()\n"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows the overall customer satisfaction level. It helps us understand how many people are happy, neutral, or unhappy with the service. This is the main target we want to improve, so it’s important to check first."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows that most customers are in the “Satisfied” group, while fewer are Neutral or Dissatisfied. This means service quality is good overall, but some customers still face issues that need improvement."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Answer Here\n","Yes, the insights will help create a positive business impact because Flipkart can see how many customers are happy and where improvements are needed. If a large number of customers are dissatisfied, it may hurt customer loyalty and lead to negative growth. By focusing on the unhappy and neutral customers, Flipkart can improve support quality, which will increase satisfaction and retention."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# -------------------------------\n","\n","plt.figure(figsize=(6,4))\n","sns.countplot(x=\"Channel\", data=df, palette=\"Set2\")\n","\n","plt.title(\"Distribution of Support Channels\", fontsize=14)\n","plt.xlabel(\"Support Channel\")\n","plt.ylabel(\"Number of Customers\")\n","plt.show()\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Answer Here.\n","ChatGPT said:\n","\n","I picked this chart because it shows which customer service channel (Chat, Email, Phone, App) is used the most. This helps us know how customers prefer to contact Flipkart support."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows which channel customers use the most for support. If one channel like Chat or Phone is much higher, it means customers find it more comfortable or faster. The less-used channels may not be as effective or preferred"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Answer Here\n","Yes, these insights will help in creating a positive business impact because Flipkart can put more resources on the most used channel to serve customers better. If some channels are very less used, it may show they are not effective or user-friendly. Ignoring this can cause negative growth as customers may feel limited in support options."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# -------------------------------\n","# Chart 3 - Distribution of Issue Categories\n","# -------------------------------\n","\n","plt.figure(figsize=(7,4))\n","sns.countplot(x=\"Category\", data=df, palette=\"Set3\")\n","\n","plt.title(\"Distribution of Customer Issue Categories\", fontsize=14)\n","plt.xlabel(\"Issue Category\")\n","plt.ylabel(\"Number of Complaints\")\n","plt.xticks(rotation=30)\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows which type of problem (Delivery, Refund, Payment, Product Quality, Technical) customers face most. It helps to understand the main pain points in customer service."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows which issue is reported the most by customers. If Delivery or Refund issues are higher, it means many customers face problems in these areas, while other issues are less common."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Answer Here\n","Yes, these insights will help create a positive business impact because Flipkart can focus on fixing the most common problems first, like Delivery or Refund issues. If these problems are not solved, more customers will be unhappy, which can lead to negative growth and loss of trust."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","# -------------------------------\n","\n","plt.figure(figsize=(7,4))\n","sns.histplot(df[\"Response_Time\"], bins=20, kde=True, color=\"skyblue\")\n","\n","plt.title(\"Distribution of Customer Service Response Time\", fontsize=14)\n","plt.xlabel(\"Response Time (in Hours)\")\n","plt.ylabel(\"Number of Complaints\")\n","plt.show()\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows how quickly customer complaints are answered. Response time is directly linked to customer satisfaction, so it is important to check."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows how response times are spread. If most responses are within a short time, it means service is quick. If many responses take longer, it shows delays in handling complaints."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["\n","Yes, the insights will help create a positive business impact because Flipkart can see if response times are too long and take action to speed them up. Quick responses improve customer trust and satisfaction, but long delays can make customers unhappy and lead to negative growth."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# -------------------------------\n","\n","plt.figure(figsize=(6,4))\n","sns.countplot(x=\"Customer_Rating\", data=df, palette=\"coolwarm\")\n","\n","plt.title(\"Distribution of Customer Ratings\", fontsize=14)\n","plt.xlabel(\"Customer Rating (1 = Poor, 5 = Excellent)\")\n","plt.ylabel(\"Number of Customers\")\n","plt.show()\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows how customers rate their service experience on a scale of 1 to 5. Ratings are a direct reflection of customer satisfaction, so it is important to analyze them"],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows how customers give ratings. If most ratings are 4 or 5, it means many customers are happy. If many ratings are 1 or 2, it shows dissatisfaction and service problems."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Answer Here\n","Yes, the insights will help create a positive business impact because Flipkart can see if customers are mostly giving high or low ratings. High ratings mean good service, but low ratings show dissatisfaction. If low ratings increase, it can hurt brand trust and cause negative growth."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","plt.figure(figsize=(6,4))\n","sns.countplot(data=df, x='Resolution_Status', palette='Set2')\n","plt.title('Resolution Status Distribution')\n","plt.xlabel('Resolution Status')\n","plt.ylabel('Number of Tickets')\n","plt.show()\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows how many customer issues were resolved versus unresolved. This helps Flipkart understand how effective their support team is."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Answer Here\n","The chart shows how many tickets were resolved and how many were not. If most tickets are resolved, it means the support team is performing well. If many tickets are unresolved, it indicates delays or issues in customer service."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Answer Here\n","If most tickets are resolved quickly, it improves customer satisfaction and loyalty, leading to positive growth.\n","\n","If many tickets remain unresolved, it can frustrate customers, lower satisfaction scores, and harm Flipkart’s brand reputation, causing negative growth."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","plt.figure(figsize=(8,5))\n","avg_response = df.groupby('Channel')['Response_Time'].mean().reset_index()\n","sns.barplot(data=avg_response, x='Channel', y='Response_Time', palette='Set3')\n","plt.title('Average Response Time by Channel')\n","plt.xlabel('Customer Support Channel')\n","plt.ylabel('Average Response Time (hours)')\n","plt.show()\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this bar chart because it clearly shows the average response time for each support channel, making it easy to compare which channels are faster or slower in handling customer queries."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["Answer Here\n","From the chart, we can see which support channels respond faster and which take longer. For example:\n","\n","Channels like Chat may have the quickest response time.\n","\n","Channels like Email or Phone may take longer to respond."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["Answer Here\n","Yes, the insights can help create a positive business impact:\n","\n","By focusing on slower channels, Flipkart can reduce response times, improving customer satisfaction and loyalty.\n","\n","Faster resolution leads to higher CSAT scores and repeat purchases.\n","\n","Negative growth possibility:\n","\n","If certain channels consistently take too long (like Email), it may cause customer dissatisfaction, complaints, or churn. Addressing this is crucial to avoid negative business impact."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","plt.figure(figsize=(10,6))\n","sns.countplot(data=df, x='Category', hue='Satisfaction_Level', palette='Set2')\n","plt.title(\"Customer Satisfaction Level by Complaint Category\")\n","plt.xlabel(\"Complaint Category\")\n","plt.ylabel(\"Number of Customers\")\n","plt.legend(title='Satisfaction Level')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this count plot because it clearly shows the number of customers in each satisfaction level for different complaint categories. It helps to quickly compare which types of complaints lead to satisfied, neutral, or dissatisfied customers."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Answer Here\n","Most customers are satisfied with simple issues like Payment or Delivery.\n","\n","Complaints related to Product Quality or Technical issues have a higher number of dissatisfied or neutral customers.\n","\n","This shows that some complaint categories need more attention to improve customer satisfaction."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["Answer Here\n","the insights will help create a positive business impact.\n","\n","Flipkart can focus on categories where customers are more dissatisfied (like Delivery or Refund) and improve those processes.\n","\n","If these issues are not solved, they can cause negative growth because unhappy customers may stop shopping or switch to competitors.\n","\n","On the other hand, improving weak areas will boost customer satisfaction, loyalty, and retention."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["\n","plt.figure(figsize=(8,5))\n","sns.boxplot(data=df, x='Satisfaction_Level', y='Response_Time', palette='pastel')\n","plt.title(\"Response Time vs Satisfaction Level\")\n","plt.xlabel(\"Satisfaction Level\")\n","plt.ylabel(\"Response Time (hours)\")\n","plt.show()\n"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["Answer Here.\n","I picked this chart because it shows how response time changes for different satisfaction levels. It helps to see if slower responses are linked with dissatisfied customers."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Answer Here\n","The insight from this chart is that customers with shorter response times are mostly satisfied, while long response times are linked with more dissatisfied customers. This shows response time strongly impacts customer satisfaction."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["Answer Here\n","these insights will help create a positive business impact because they show that faster response times lead to higher customer satisfaction. By reducing delays, Flipkart can improve loyalty and trust. On the other hand, longer response times can lead to negative growth, as dissatisfied customers may stop using the service or give bad reviews, which can harm brand image."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","plt.figure(figsize=(8,5))\n","sns.histplot(df[\"Customer_Rating\"], bins=5, kde=True, color=\"skyblue\", edgecolor=\"black\")\n","\n","plt.title(\"Distribution of Customer Ratings\", fontsize=14, fontweight=\"bold\")\n","plt.xlabel(\"Customer Rating (1 = Poor, 5 = Excellent)\")\n","plt.ylabel(\"Count\")\n","plt.show()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["\n","I picked this chart because a histogram is the best way to show the distribution of customer ratings. It clearly highlights how many customers gave low, medium, or high ratings, making it easy to understand overall satisfaction levels."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["\n","The insight from the chart is that most customers gave higher ratings (4–5), which means they are generally satisfied. A smaller group gave low ratings (1–2), showing there are still some unhappy customers who need attention."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["Answer Here\n","the insights can create a positive business impact because knowing that most customers are satisfied means the company is doing well in customer service.\n","\n","However, the negative side is that low ratings still exist, which shows gaps in service quality. If these unhappy customers are not addressed, it may lead to negative growth through complaints, cancellations, or negative reviews."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","try:\n","    if \"Agent_ID\" not in df.columns:\n","        raise KeyError(\"Agent_ID column not found in dataframe.\")\n","\n","    # Compute agent stats\n","    agent_stats = (\n","        df.groupby(\"Agent_ID\")\n","          .agg(ticket_count=(\"Customer_ID\", \"count\"),\n","               avg_rating=(\"Customer_Rating\", \"mean\"))\n","          .reset_index()\n","          .sort_values(by=\"ticket_count\", ascending=False)\n","    )\n","\n","    top_agents = agent_stats.head(10).iloc[::-1]  # reverse for horizontal bar plot\n","\n","    plt.figure(figsize=(10,6))\n","    bars = plt.barh(top_agents[\"Agent_ID\"].astype(str), top_agents[\"ticket_count\"])\n","    plt.xlabel(\"Number of Tickets\")\n","    plt.title(\"Chart 11 — Top 10 Agents by Ticket Count (with avg rating labels)\")\n","\n","    # Annotate bars with avg rating\n","    for bar, rating in zip(bars, top_agents[\"avg_rating\"].round(2)):\n","        w = bar.get_width()\n","        plt.text(w + 1, bar.get_y() + bar.get_height()/2,\n","                 f\"Avg Rating: {rating}\", va='center', fontsize=9)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","except KeyError as ke:\n","    print(\"Skipped Chart 11 - reason:\", ke)\n","except Exception as e:\n","    print(\"Error while creating Chart 11:\", e)\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["\n","\n","I picked this chart because it shows the top-performing agents by ticket volume and links it with their average customer rating. It helps to understand workload distribution and service quality at the same time.Answer Here."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["Answer Here\n","The insight found is that some agents handle a high number of tickets but still maintain good ratings, which shows efficiency and skill. On the other hand, a few agents manage fewer tickets yet receive lower ratings, which highlights possible training or performance issues."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["these insights will help create a positive business impact because Flipkart can:\n","\n","Identify top-performing agents and use them as benchmarks or mentors.\n","\n","Detect underperforming agents early and provide them with extra training or process support.\n","\n","Balance ticket allocation so that customer experience remains consistent.\n","\n"," Negative growth risk: If low-performing agents are not improved or replaced, it can lead to higher dissatisfaction, customer churn, and damage to brand trust."],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart 12 - Countplot of Movies vs TV Shows\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(7,5))\n","sns.countplot(data=df, x=\"Category\", palette=\"viridis\")\n","\n","plt.title(\"Distribution of Movies vs TV Shows on Amazon Prime\", fontsize=14, fontweight='bold')\n","plt.xlabel(\"Content Type\", fontsize=12)\n","plt.ylabel(\"Count\", fontsize=12)\n","plt.xticks(rotation=0)\n","plt.show()\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["I picked a countplot because it is the simplest and most effective way to compare the frequency of categorical values (Movies vs TV Shows). It clearly shows the distribution in one glance and makes it easy to identify which content type dominates Amazon Prime’s library.Answer Here."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["The chart shows that Amazon Prime has a higher number of Movies compared to TV Shows (or vice versa, depending on dataset). This highlights Prime’s stronger focus on movies, while TV Shows form a smaller portion of the library.Answer Here"],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Positive Impact: Knowing that Movies dominate the library can help Amazon focus marketing and recommendation strategies for movie lovers, attracting more subscribers.\n","\n","Negative Insight: If TV Shows are very low, it might limit engagement for binge-watchers, giving competitors like Netflix or Disney+ an advantage in retaining users who prefer series.\n","\n","This insight helps Amazon balance content and make data-driven decisions for future acquisitions.Answer Here"],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["try:\n","    genre_column = None\n","    # Try common names for genre column\n","    for col in ['genres', 'Genres', 'listed_in', 'category']:\n","        if col in df.columns:\n","            genre_column = col\n","            break\n","\n","    if genre_column:\n","        # Split multiple genres per row and flatten\n","        all_genres = df[genre_column].dropna().str.split(\",\").explode().str.strip()\n","        top_genres = all_genres.value_counts().nlargest(10)\n","\n","        plt.figure(figsize=(10,6))\n","        sns.barplot(x=top_genres.values, y=top_genres.index, palette=\"magma\")\n","        plt.title(\"Top 10 Genres on Amazon Prime\", fontsize=14, fontweight='bold')\n","        plt.xlabel(\"Count\", fontsize=12)\n","        plt.ylabel(\"Genre\", fontsize=12)\n","        plt.show()\n","    else:\n","        print(\"Genre column not found!\")\n","except Exception as e:\n","    print(\"Error in Chart 13:\", e)"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":[" (Movies vs TV Shows): Picked a countplot to easily compare the number of Movies and TV Shows. It quickly shows which type of content dominates Amazon Prime’s library.\n","\n","Chart 13 (Top 10 Genres): Picked a barplot to highlight the most common genres. It helps understand which genres Amazon Prime focuses on and what content users are most likely to watch."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["Answer Here\n","Amazon Prime has more Movies than TV Shows (or vice versa depending on your dataset).\n","\n","This indicates the platform’s stronger focus on one type of content.\n","\n","Chart 13 (Top 10 Genres):\n","\n","The top genres (like Drama, Comedy, Action, etc.) dominate the library.\n","\n","Less frequent genres show areas where content is limited, revealing opportunities for expansion.\n","\n","Do you want me to also provide Q3 (Business Impact) for both charts in short and simple terms?"],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["\n","Chart 13 (Top 10 Genres):\n","\n","Positive Impact: Identifying popular genres helps Amazon plan future acquisitions and promotions for high-demand content.\n","\n","Negative Insight: Less represented genres may lose niche audiences, reducing engagement in those segments.\n","\n","These insights enable Amazon to balance its content library and make data-driven decisions to maximize user retention and growth."],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","try:\n","    # Select only numeric columns\n","    numeric_df = df.select_dtypes(include=['int64', 'float64'])\n","\n","    if not numeric_df.empty:\n","        plt.figure(figsize=(10,8))\n","        corr = numeric_df.corr()\n","        sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n","        plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=14, fontweight='bold')\n","        plt.show()\n","    else:\n","        print(\"No numeric columns found for correlation heatmap!\")\n","except Exception as e:\n","    print(\"Error in Correlation Heatmap:\", e)\n"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["Answer Here.To understand relationships between numeric features, which can help identify patterns or redundant features."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["Answer Here\n","Features with high correlation can indicate redundancy (e.g., duration and episode_count).\n","\n","Low or negative correlations reveal independent attributes that affect user engagement differently."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","try:\n","    # Select only numeric columns\n","    numeric_df = df.select_dtypes(include=['int64', 'float64'])\n","\n","    if not numeric_df.empty:\n","        sns.pairplot(numeric_df)\n","        plt.suptitle(\"Pair Plot of Numeric Features\", fontsize=14, fontweight='bold', y=1.02)\n","        plt.show()\n","    else:\n","        print(\"No numeric columns found for pair plot!\")\n","except Exception as e:\n","    print(\"Error in Pair Plot:\", e)\n"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["Answer Here.\n","To visually explore relationships and distributions between numeric features, spotting trends, clusters, or outliers."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["Answer Here\n","Identify positive or negative correlations between features.\n","\n","Spot outliers or unusual patterns in the dataset."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 (Content Type):\n","\n","H0 (Null): There is no significant difference in the average duration of Movies and TV Shows.\n","\n","H1 (Alternative): Movies and TV Shows have significantly different average durations.\n","\n","Reasoning: From Chart 12, we saw distribution of Movies vs TV Shows. Testing duration differences helps understand content design strategy.\n","\n","Hypothesis 2 (Genre Popularity vs Ratings):\n","\n","H0: The average ratings of the top 3 most common genres are equal.\n","\n","H1: At least one genre has a significantly different average rating.\n","\n","Reasoning: From Chart 13 (Top 10 Genres), we can test if popularity correlates with user ratings.\n","\n","Hypothesis 3 (Release Year vs Ratings):\n","\n","H0: There is no correlation between release year and ratings.\n","\n","H1: Release year and ratings are correlated.\n","\n","Reasoning: From numeric features analysis (heatmap/pairplot), we can check if newer content is rated higher or lower."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["H0 (Null): There is no significant difference in the average duration of Movies and TV Shows.\n","\n","H1 (Alternative): Movies and TV Shows have significantly different average durations.\n","\n","Reasoning: From Chart 12, we saw distribution of Movies vs TV Shows. Testing duration differences helps understand content design strategy."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["import pandas as pd\n","import scipy.stats as stats\n","\n","# 1. Hypothesis 1: Average duration of Movies vs TV Shows\n","try:\n","    # Identify content type column\n","    content_col = None\n","    for col in ['type','Type','Category','content_type','show_type']:\n","        if col in df.columns:\n","            content_col = col\n","            break\n","\n","    # Identify duration column\n","    duration_col = None\n","    for col in ['duration','Duration','runtime','length']:\n","        if col in df.columns:\n","            duration_col = col\n","            break\n","\n","    if content_col and duration_col:\n","        movies_duration = df[df[content_col].str.contains(\"Movie\", case=False, na=False)][duration_col].dropna()\n","        tv_duration = df[df[content_col].str.contains(\"TV\", case=False, na=False)][duration_col].dropna()\n","\n","        t_stat, p_val = stats.ttest_ind(movies_duration, tv_duration, equal_var=False)\n","        print(\"Hypothesis 1 - Movies vs TV Shows duration\")\n","        print(\"T-statistic:\", t_stat)\n","        print(\"P-value:\", p_val)\n","    else:\n","        print(\"Required columns for Hypothesis 1 not found.\")\n","except Exception as e:\n","    print(\"Error in Hypothesis 1:\", e)\n","\n","# 2. Hypothesis 2: Average ratings of top 3 genres\n","try:\n","    # Identify genre and rating columns\n","    genre_col = None\n","    rating_col = None\n","    for col in ['genres','Genres','listed_in','category']:\n","        if col in df.columns:\n","            genre_col = col\n","            break\n","    for col in ['rating','Rating','score','Rating_Score']:\n","        if col in df.columns:\n","            rating_col = col\n","            break\n","\n","    if genre_col and rating_col:\n","        # Top 3 genres\n","        all_genres = df[genre_col].dropna().str.split(\",\").explode().str.strip()\n","        top3_genres = all_genres.value_counts().nlargest(3).index.tolist()\n","\n","        ratings_data = []\n","        for genre in top3_genres:\n","            genre_ratings = df[df[genre_col].str.contains(genre, na=False)][rating_col].dropna()\n","            ratings_data.append(genre_ratings)\n","\n","        f_stat, p_val2 = stats.f_oneway(*ratings_data)\n","        print(\"\\nHypothesis 2 - Ratings across top 3 genres\")\n","        print(\"F-statistic:\", f_stat)\n","        print(\"P-value:\", p_val2)\n","    else:\n","        print(\"Required columns for Hypothesis 2 not found.\")\n","except Exception as e:\n","    print(\"Error in Hypothesis 2:\", e)\n","\n","# 3. Hypothesis 3: Correlation between release year and ratings\n","try:\n","    # Identify release year column\n","    year_col = None\n","    for col in ['release_year','Release_Year','year']:\n","        if col in df.columns:\n","            year_col = col\n","            break\n","\n","    if year_col and rating_col:\n","        release_year = df[year_col].dropna()\n","        ratings = df[rating_col].dropna()\n","        # Align indices\n","        common_index = release_year.index.intersection(ratings.index)\n","        release_year = release_year.loc[common_index]\n","        ratings = ratings.loc[common_index]\n","\n","        corr_coef, p_val3 = stats.pearsonr(release_year, ratings)\n","        print(\"\\nHypothesis 3 - Correlation between Release Year and Ratings\")\n","        print(\"Correlation coefficient:\", corr_coef)\n","        print(\"P-value:\", p_val3)\n","    else:\n","        print(\"Required columns for Hypothesis 3 not found.\")\n","except Exception as e:\n","    print(\"Error in Hypothesis 3:\", e)\n"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 (Movies vs TV Shows duration): Independent t-test – compares mean duration of two groups.\n","\n","Hypothesis 2 (Ratings across top 3 genres): One-way ANOVA – compares mean ratings of more than two groups.\n","\n","Hypothesis 3 (Release Year vs Ratings): Pearson correlation – checks linear relationship between two numeric variables.\n","\n","p-value < 0.05: Significant → reject H0\n","\n","p-value ≥ 0.05: Not significant → fail to reject H0"],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 – t-test: Because we are comparing the mean duration of two independent groups (Movies vs TV Shows).\n","\n","Hypothesis 2 – One-way ANOVA: Because we are comparing the mean ratings of more than two groups (top 3 genres).\n","\n","Hypothesis 3 – Pearson correlation: Because we want to measure the linear relationship between two numeric variables (release year and ratings).\n","\n","Each test matches the type of data and the number of groups/variables being analyzed."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n","Hypothesis 1 – Movies vs TV Shows Duration\n","\n","H0: There is no significant difference in the average duration of Movies and TV Shows.\n","\n","H1: Movies and TV Shows have significantly different average durations.\n","\n","Hypothesis 2 – Ratings Across Top 3 Genres\n","\n","H0: The average ratings of the top 3 genres are equal.\n","\n","H1: At least one genre has a significantly different average rating.\n","\n","Hypothesis 3 – Release Year vs Ratings\n","\n","H0: There is no correlation between release year and ratings.\n","\n","H1: There is a significant correlation between release year and ratings."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["print(df.columns)"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 (Movies vs TV Shows duration): Independent t-test – compares the mean duration of two independent groups (Movies vs TV Shows).\n","\n","Hypothesis 2 (Ratings across top 3 genres): One-way ANOVA – compares the mean ratings across more than two groups (top 3 genres).\n","\n","Hypothesis 3 (Release Year vs Ratings correlation): Pearson correlation test – measures the linear relationship between two numeric variables (release year and ratings).\n","\n","p-value < 0.05: Reject null hypothesis (significant difference/correlation)\n","\n","p-value ≥ 0.05: Fail to reject null hypothesis (not significant)"],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 – Independent t-test: Because we are comparing the mean duration of two independent groups (Movies vs TV Shows).\n","\n","Hypothesis 2 – One-way ANOVA: Because we are comparing the mean ratings of more than two groups (top 3 genres).\n","\n","Hypothesis 3 – Pearson correlation: Because we want to measure the linear relationship between two numeric variables (release year and ratings).\n","\n","Each test matches the data type and number of groups/variables being analyzed."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Answer Here.\n","Hypothesis 1 – Movies vs TV Shows Duration\n","\n","H0: There is no significant difference in the average duration of Movies and TV Shows.\n","\n","H1: Movies and TV Shows have significantly different average durations.\n","\n","Hypothesis 2 – Ratings Across Top 3 Genres\n","\n","H0: The average ratings of the top 3 genres are equal.\n","\n","H1: At least one genre has a significantly different average rating.\n","\n","Hypothesis 3 – Release Year vs Ratings\n","\n","H0: There is no correlation between release year and ratings.\n","\n","H1: There is a significant correlation between release year and ratings."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Show all column names\n","print(df.columns)\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["Answer Here.\n","Independent t-test – To compare the mean duration between two independent groups: Movies vs TV Shows.\n","\n","One-way ANOVA – To compare the mean ratings across more than two genres (top 3 genres).\n","\n","Pearson correlation – To check the linear relationship between release year and ratings.\n","\n","These tests were chosen based on the data type and number of groups/variables being analyzed."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["Answer Here.\n","Independent t-test → Used because we are comparing the means of a continuous variable (duration) between two independent groups: Movies vs TV Shows.\n","\n","One-way ANOVA → Used because we are comparing the means of a continuous variable (ratings) across more than two independent groups: top 3 genres.\n","\n","Pearson correlation → Used because we are testing the linear relationship between two continuous variables: release year and ratings."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Check all column names in your dataset\n","print(df.columns)\n","\n","# Remove extra spaces and standardize column names\n","df.columns = df.columns.str.strip()  # removes leading/trailing spaces\n","df.columns = df.columns.str.lower()  # optional: convert all to lowercase\n","\n","# Now check for missing values\n","print(df.isnull().sum())\n","\n","# Example of imputation with corrected column name\n","if 'duration' in df.columns:\n","    df['duration'] = df['duration'].fillna(df['duration'].median())\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["Answer Here.\n","Numerical columns: Filled with median (robust to outliers).\n","\n","Categorical columns: Filled with mode or 'Unknown' (keeps common category).\n","\n","Drop rows: Only if very few missing values.\n","\n","Ensures data integrity without losing much information or skewing results."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["print(df.columns)"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["\n","It identifies outliers by calculating the lower and upper bounds:\n","\n","Lower bound\n","=\n","𝑄\n","1\n","−\n","1.5\n","×\n","𝐼\n","𝑄\n","𝑅\n",",\n","Upper bound\n","=\n","𝑄\n","3\n","+\n","1.5\n","×\n","𝐼\n","𝑄\n","𝑅\n","Lower bound=Q1−1.5×IQR,Upper bound=Q3+1.5×IQR\n","\n","Any value outside this range is considered an outlier.\n","\n","I clipped the outliers to the nearest boundary to reduce their effect without removing data.\n","\n","Reason for using this:\n","\n","Preserves most of the data.\n","\n","Reduces distortion in statistical analysis and visualization caused by extreme values.\n","\n","Simple and effective for skewed distributions."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# 1. Check all column names\n","print(df.columns)\n","\n","# 2. Identify categorical columns automatically\n","categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n","print(\"Categorical columns:\", categorical_cols)\n","\n","# 3. Encode categorical columns\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    df[col + '_encoded'] = le.fit_transform(df[col])\n","\n","# 4. (Optional) Drop original categorical columns\n","# df.drop(columns=categorical_cols, inplace=True)\n","\n","# 5. Check the result\n","df.head()\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["Label Encoding – Converts categorical text values into numerical labels.\n","\n","Why used: Simple and efficient for ordinal categories (where order matters, e.g., ratings).\n","\n","One-Hot Encoding – Converts each category into a separate binary column (0 or 1).\n","\n","Why used: Prevents numeric misinterpretation of non-ordinal categories (e.g., genres, country).\n","\n"," Using these ensures machine learning models can process categorical data correctly without introducing bias."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","Textual Data Preprocessing Steps:\n","\n","Lowercasing – Convert all text to lowercase to maintain uniformity.\n","\n","Removing Punctuation & Special Characters – Clean text for better analysis.\n","\n","Tokenization – Split sentences into words or tokens.\n","\n","Stopwords Removal – Remove common words (e.g., “the”, “is”) that don’t add meaning.\n","\n","Stemming / Lemmatization – Reduce words to their root form (e.g., “running” → “run”).\n","\n","Handling Missing Text – Fill or remove null/empty text entries.\n","\n"," This ensures the textual dataset is clean, consistent, and ready for NLP tasks like sentiment analysis, text clustering, or topic modeling."],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["!pip install contractions\n","\n","import pandas as pd\n","import contractions\n","\n","# Sample dataset\n","data = pd.DataFrame({'text': [\"I can't do this\", \"She won't go there\"]})\n","\n","# Expand contractions\n","data['text_expanded'] = data['text'].apply(lambda x: contractions.fix(x))\n","\n","print(data)\n"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","import pandas as pd\n","\n","# Sample dataset\n","data = pd.DataFrame({'text': [\"I Love Python\", \"Data Science is Fun\"]})\n","\n","# Convert text to lowercase\n","data['text_lower'] = data['text'].str.lower()\n","\n","print(data)\n"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import pandas as pd\n","import string\n","\n","# Sample dataset\n","data = pd.DataFrame({'text': [\"I love Python!\", \"Data Science is fun, right?\"]})\n","\n","# Remove punctuations\n","data['text_clean'] = data['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n","\n","print(data)\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","import pandas as pd\n","import re\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'text': [\n","        \"Check this link: https://example.com\",\n","        \"The model version is v2.0 and it works well\",\n","        \"Call me at 123-456-7890\"\n","    ]\n","})\n","\n","# Remove URLs\n","data['text_clean'] = data['text'].str.replace(r'http\\S+|www.\\S+', '', regex=True)\n","\n","# Remove words containing digits\n","data['text_clean'] = data['text_clean'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n","\n","# Remove extra spaces\n","data['text_clean'] = data['text_clean'].str.strip()\n","\n","print(data)\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n","import pandas as pd\n","from nltk.corpus import stopwords\n","import nltk\n","\n","# Download stopwords\n","nltk.download('stopwords')\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'text_clean': [\n","        \"Check this link\",\n","        \"The model version is and it works well\",\n","        \"Call me at\"\n","    ]\n","})\n","\n","# Define stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Remove stopwords\n","data['text_clean'] = data['text_clean'].apply(\n","    lambda x: ' '.join(word for word in x.split() if word.lower() not in stop_words)\n",")\n","\n","print(data)\n"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","import pandas as pd\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'text_clean': [\n","        \"  Check   link  \",\n","        \"model   version   works well \",\n","        \"   Call  \"\n","    ]\n","})\n","\n","# Remove leading, trailing and extra spaces\n","data['text_clean'] = data['text_clean'].str.strip()  # Remove leading/trailing spaces\n","data['text_clean'] = data['text_clean'].replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with single space\n","\n","print(data)\n"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n","from textblob import TextBlob\n","import pandas as pd\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'text_clean': [\n","        \"I am loving this movie!\",\n","        \"This show is not good.\",\n","        \"The acting was amazing.\"\n","    ]\n","})\n","\n","# Rephrase/Normalize text using TextBlob (correct grammar/spelling)\n","data['text_rephrased'] = data['text_clean'].apply(lambda x: str(TextBlob(x).correct()))\n","\n","print(data)\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Download the required NLTK data\n","nltk.download('punkt_tab')\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'text_rephrased': [\n","        \"I am loving this movie!\",\n","        \"This show is not good.\",\n","        \"The acting was amazing.\"\n","    ]\n","})\n","\n","# Tokenization\n","data['tokens'] = data['text_rephrased'].apply(word_tokenize)\n","\n","print(data)\n"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Sample data\n","data = pd.DataFrame({'text': [\"I can't believe this movie is amazing!!!\"]})\n","\n","# Initialize lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","def normalize_text(text):\n","    text = text.lower()  # Lowercase\n","    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove punctuation & digits\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]  # Lemmatize & remove stopwords\n","    return \" \".join(words)\n","\n","data['text_normalized'] = data['text'].apply(normalize_text)\n","print(data)\n"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["Answer Here.\n","Lowercasing – To make text uniform and avoid treating the same word differently due to case.\n","\n","Removing punctuation & digits – To clean the text and focus only on meaningful words.\n","\n","Removing stopwords – To eliminate common words that do not add value to analysis.\n","\n","Lemmatization – To reduce words to their base form (e.g., “running” → “run”) so similar words are treated the same.\n","\n","Reason: These techniques standardize text, reduce noise, and make it suitable for NLP tasks like text analysis, clustering, or sentiment analysis."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["import nltk\n","\n","# Download the specific tagger NLTK is asking for\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# Sample text\n","text = \"Amazon Prime Video has a wide variety of movies and TV shows.\"\n","\n","# Tokenize\n","tokens = word_tokenize(text)\n","\n","# POS tagging\n","pos_tags = pos_tag(tokens)\n","print(pos_tags)\n"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Check all column names\n","print(data.columns)\n"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["Answer Here.\n","It converts text into numerical features suitable for machine learning.\n","\n","It considers both the frequency of a word in a document (term frequency) and how unique the word is across all documents (inverse document frequency).\n","\n","Helps reduce the weight of common words (like “the”, “is”) and highlights important words for analysis.\n","\n","Works well for tasks like text classification, clustering, and NLP analysis."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation\n","Feature Creation:\n","\n","Combine existing features or extract new information.\n","\n","Example: From release_date, create release_year or month.\n","\n","Feature Transformation:\n","\n","Scale or normalize numeric features (MinMaxScaler, StandardScaler).\n","\n","Apply log/Box-Cox transformation to reduce skewness.\n","\n","Feature Encoding:\n","\n","Convert categorical features into numeric (Label Encoding, One-Hot Encoding).\n","\n","Feature Extraction from Text:\n","\n","Extract keywords, word counts, or sentiment scores.\n","\n","Feature Selection:\n","\n","Remove irrelevant or redundant features using correlation, variance threshold, or model-based selection."],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","import pandas as pd\n","import numpy as np\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'views': [100, 150, 200, 250, 300],\n","    'likes': [10, 20, 25, 30, 40],\n","    'dislikes': [1, 2, 2, 3, 4],\n","    'duration_min': [5, 10, 15, 20, 25]\n","})\n","\n","# 1. Create new feature: like_ratio\n","data['like_ratio'] = data['likes'] / (data['likes'] + data['dislikes'])\n","\n","# 2. Reduce correlation: log-transform highly correlated feature 'views'\n","data['log_views'] = np.log1p(data['views'])\n","\n","# 3. Create feature from existing features: engagement score\n","data['engagement'] = data['likes'] + data['dislikes']\n","\n","print(data)\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.feature_selection import SelectFromModel\n","\n","# Sample dataset\n","data = pd.DataFrame({\n","    'views': [100, 150, 200, 250, 300],\n","    'likes': [10, 20, 25, 30, 40],\n","    'dislikes': [1, 2, 2, 3, 4],\n","    'duration_min': [5, 10, 15, 20, 25],\n","    'engagement': [11, 22, 27, 33, 44]  # target\n","})\n","\n","X = data[['views', 'likes', 'dislikes', 'duration_min']]\n","y = data['engagement']\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Feature selection using Random Forest\n","model = RandomForestRegressor()\n","model.fit(X_train, y_train)\n","\n","# Select important features\n","selector = SelectFromModel(model, prefit=True)\n","X_train_selected = selector.transform(X_train)\n","X_test_selected = selector.transform(X_test)\n","\n","print(\"Selected features shape:\", X_train_selected.shape)\n"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Answer Here.\n","Correlation Analysis – To remove highly correlated features and reduce multicollinearity.\n","\n","Random Forest / Tree-based Feature Importance – To automatically identify the most important features for prediction.\n","\n","SelectFromModel – To select only the significant features based on model importance scores.\n","\n","Domain Knowledge / Manual Selection – To keep features that are meaningful for business or prediction, avoiding irrelevant ones.\n","Reduces overfitting by eliminating unnecessary or redundant features.\n","\n","Improves model performance and interpretability.\n","\n","Helps in faster training and easier maintenance of the model."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Answer Here.\n","Release Year / Age of Content – Older or newer shows/movies may influence user engagement or popularity.\n","\n","Genre – Certain genres attract more viewers and affect recommendation or view patterns.\n","\n","Duration / Episode Count – Longer movies or shorter series can impact user completion rates.\n","\n","Rating – High-rated content generally attracts more viewers.\n","\n","Type (Movie / TV Show) – User preferences vary between movies and series.\n","\n","Cast / Director / Country (if available) – Popular actors, directors, or regional content can drive engagement.      \n","These features directly affect user behavior and content performance.\n","\n","They have strong correlations with target variables like user rating, watch time, or engagement.\n","\n","Including these improves prediction accuracy and business insight generation."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n","Skewed Numerical Data – Features like duration or views often have long tails, which can bias models.\n","\n","Categorical Data – Textual categories like genre or type needed to be converted into numerical form for ML models.\n","\n","Text Data – Descriptions or titles needed cleaning, normalization, and vectorization for NLP tasks."],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Load dataset\n","# data = pd.read_csv('your_dataset.csv')\n","\n","# Ensure columns exist\n","numerical_cols = [col for col in ['duration', 'views'] if col in data.columns]\n","categorical_cols = [col for col in ['type', 'genre'] if col in data.columns]\n","text_col = 'description' if 'description' in data.columns else None\n","\n","# 1. Log Transformation for skewed numerical columns\n","for col in numerical_cols:\n","    data[col] = data[col].apply(lambda x: np.log1p(x))\n","\n","# 2. Standard Scaling\n","scaler = StandardScaler()\n","if numerical_cols:\n","    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n","\n","# 3. One-Hot Encoding\n","if categorical_cols:\n","    encoder = OneHotEncoder(sparse=False, drop='first')\n","    encoded_data = pd.DataFrame(encoder.fit_transform(data[categorical_cols]),\n","                                columns=encoder.get_feature_names_out(categorical_cols),\n","                                index=data.index)\n","    data = data.drop(categorical_cols, axis=1)\n","    data = pd.concat([data, encoded_data], axis=1)\n","\n","# 4. Text Vectorization\n","if text_col:\n","    tfidf = TfidfVectorizer(max_features=500)\n","    text_features = pd.DataFrame(tfidf.fit_transform(data[text_col].fillna('')).toarray(),\n","                                 columns=tfidf.get_feature_names_out(),\n","                                 index=data.index)\n","    data = data.drop(text_col, axis=1)\n","    data = pd.concat([data, text_features], axis=1)\n","\n","print(data.head())\n"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Check current column names\n","print(data.columns)\n","\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?\n","It centers the data around 0 by subtracting the mean and dividing by the standard deviation.\n","\n","It ensures that features with different ranges or units don’t dominate the model.\n","\n","It works well for algorithms that rely on distance or gradients, like Linear Regression, Logistic Regression, SVM, and Neural Networks."],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Correct numerical columns\n","numerical_cols = ['views', 'likes', 'dislikes', 'duration_min', 'engagement']\n","\n","# Initialize PCA (e.g., reduce to 2 components)\n","pca = PCA(n_components=2)\n","\n","# Apply PCA on the correct numerical columns\n","data_num_reduced = pca.fit_transform(data[numerical_cols])\n","\n","# Convert to DataFrame\n","data_num_reduced = pd.DataFrame(data_num_reduced, columns=['PC1', 'PC2'])\n","print(data_num_reduced.head())\n"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["Answer Here.\n","It reduces the number of features while retaining most of the variance (information) in the dataset.\n","\n","Helps to remove multicollinearity between numerical features.\n","\n","Makes the dataset less complex, reducing the risk of overfitting in machine learning models.\n","\n","Speeds up computation by working with fewer transformed features (principal components)."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Define features (X) and target (y)\n","X = data.drop('engagement', axis=1)   # Features\n","y = data['engagement']                # Target\n","\n","# Split into 80% train and 20% test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(\"Train shape:\", X_train.shape, y_train.shape)\n","print(\"Test shape:\", X_test.shape, y_test.shape)\n"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["Answer Here.\n","80% for training → ensures the model gets enough data to learn patterns.\n","\n","20% for testing → provides a fair portion of unseen data to evaluate performance.\n","\n","It’s a widely accepted standard in ML when the dataset is of moderate to large size, giving a good balance between training and evaluation."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = data.drop('engagement', axis=1)\n","y = data['engagement']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(\"Train size:\", X_train.shape)\n","print(\"Test size:\", X_test.shape)\n"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Answer Here.\n","Instead of simply duplicating minority samples, SMOTE creates synthetic examples by interpolating between existing ones.\n","\n","This prevents overfitting that can happen with random oversampling.\n","\n","It ensures all classes have enough representation, improving the model’s ability to learn patterns from underrepresented classes."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Fit the model\n","model1 = LogisticRegression(max_iter=1000, random_state=42)\n","model1.fit(X_train, y_train)\n","\n","# Predict\n","y_pred1 = model1.predict(X_test)\n","\n","# Evaluate\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred1))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred1))\n"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import numpy as np\n","\n","# Use only labels that are present in y_test\n","labels = np.unique(np.concatenate([y_test, y_pred1]))\n","\n","cm = confusion_matrix(y_test, y_pred1, labels=labels)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot(cmap=plt.cm.Blues)\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Example DataFrame (replace with your real dataset)\n","data = pd.DataFrame({\n","    \"views\": [1000, 2500, 4000, 8000, 12000],\n","    \"likes\": [100, 300, 500, 1000, 1500],\n","    \"dislikes\": [10, 30, 40, 50, 60],\n","    \"duration_min\": [5, 10, 15, 20, 25],\n","    \"engagement\": [0.15, 0.20, 0.25, 0.35, 0.40]\n","})\n","\n","# Features (X) and Target (y)\n","X = data.drop(\"engagement\", axis=1)\n","y = data[\"engagement\"]\n","\n","print(\"✅ Features (X):\")\n","print(X)\n","\n","print(\"\\n🎯 Target (y):\")\n","print(y)\n","\n","# Splitting into train & test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"\\n📊 X_train:\")\n","print(X_train)\n","\n","print(\"\\n📊 y_train:\")\n","print(y_train)\n"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["Answer Here.\n","Exhaustive Search – It systematically tries all combinations of hyperparameters, ensuring the best parameter set is found.\n","\n","Cross-Validation – It uses k-fold cross-validation while testing combinations, which reduces the risk of overfitting.\n","\n","Reliable for Small to Medium Search Space – Since my dataset and parameter space were not extremely large, GridSearchCV was practical and efficient.\n","\n","Direct Comparison – It gives a clear comparison of performance metrics across hyperparameter combinations."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Answer Here.\n","Exhaustive Search – It systematically tries all combinations of hyperparameters, ensuring the best parameter set is found.\n","\n","Cross-Validation – It uses k-fold cross-validation while testing combinations, which reduces the risk of overfitting.\n","\n","Reliable for Small to Medium Search Space – Since my dataset and parameter space were not extremely large, GridSearchCV was practical and efficient.\n","\n","Direct Comparison – It gives a clear comparison of performance metrics across hyperparameter combinations."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","model = RandomForestRegressor(n_estimators=10, random_state=42)\n","model.fit(X, y)\n","y_pred = model.predict(X)\n","\n","# Simple visualization of actual vs predicted\n","import matplotlib.pyplot as plt\n","plt.scatter(y, y_pred)\n","plt.xlabel(\"Actual Engagement\")\n","plt.ylabel(\"Predicted Engagement\")\n","plt.title(\"Actual vs Predicted Engagement\")\n","plt.show()\n"],"metadata":{"id":"ORbOX3m0DDX_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","# Features and target\n","X = data.drop(\"engagement\", axis=1)\n","y = data[\"engagement\"]\n","\n","# Fit the model\n","model = RandomForestRegressor(n_estimators=10, random_state=42)\n","model.fit(X, y)\n","\n","# Predict\n","y_pred = model.predict(X)\n","\n","# Calculate metrics\n","r2 = r2_score(y, y_pred)\n","mse = mean_squared_error(y, y_pred)\n","\n","print(\"R2 Score:\", r2)\n","print(\"Mean Squared Error:\", mse)\n","\n","# Visualize metric scores\n","metrics = {'R2 Score': r2, 'MSE': mse}\n","plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'salmon'])\n","plt.title(\"ML Model Evaluation Metric Scores\")\n","plt.show()\n","\n","# Optional: Actual vs Predicted Scatter Plot\n","plt.scatter(y, y_pred, color='blue')\n","plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # perfect prediction line\n","plt.xlabel(\"Actual Engagement\")\n","plt.ylabel(\"Predicted Engagement\")\n","plt.title(\"Actual vs Predicted Engagement\")\n","plt.show()\n"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","# Features and target\n","X = data.drop(\"engagement\", axis=1)\n","y = data[\"engagement\"]\n","\n","# Define model with hyperparameters (manual tuning)\n","model = RandomForestRegressor(\n","    n_estimators=50,   # number of trees\n","    max_depth=3,       # max depth of each tree\n","    random_state=42\n",")\n","\n","# Fit the algorithm\n","model.fit(X, y)\n","\n","# Predict\n","y_pred = model.predict(X)\n","\n","# Evaluate\n","r2 = r2_score(y, y_pred)\n","mse = mean_squared_error(y, y_pred)\n","\n","print(\"R2 Score:\", r2)\n","print(\"Mean Squared Error:\", mse)\n"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["Answer Here.\n","The dataset is too small for standard CV-based optimization.\n","\n","Manual tuning allows control and ensures the model can still be trained and evaluated without errors.\n","\n","When the dataset size increases, techniques like GridSearchCV, RandomizedSearchCV, or Bayesian Optimization can be applied to systematically find the best hyperparameters."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["Since your dataset is extremely small (4 rows), any changes in hyperparameters will likely have minimal or unstable impact on evaluation metrics like R² or MSE. But for demonstration, we can compare default model vs manually tuned hyperparameters."],"metadata":{"id":"Mxudh98KEqum"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","# Default model\n","model_default = RandomForestRegressor(random_state=42)\n","model_default.fit(X, y)\n","y_pred_default = model_default.predict(X)\n","\n","r2_default = r2_score(y, y_pred_default)\n","mse_default = mean_squared_error(y, y_pred_default)\n","\n","# Tuned model (manual tuning)\n","model_tuned = RandomForestRegressor(n_estimators=50, max_depth=3, random_state=42)\n","model_tuned.fit(X, y)\n","y_pred_tuned = model_tuned.predict(X)\n","\n","r2_tuned = r2_score(y, y_pred_tuned)\n","mse_tuned = mean_squared_error(y, y_pred_tuned)\n","\n","# Plotting evaluation metric scores\n","metrics = {'R2 Score': [r2_default, r2_tuned], 'MSE': [mse_default, mse_tuned]}\n","labels = ['Default', 'Tuned']\n","\n","plt.figure(figsize=(8,5))\n","for i, metric in enumerate(metrics.keys()):\n","    plt.subplot(1, 2, i+1)\n","    plt.bar(labels, metrics[metric], color=['skyblue', 'salmon'])\n","    plt.title(metric)\n","plt.suptitle(\"Model Performance: Default vs Tuned\")\n","plt.show()\n"],"metadata":{"id":"4DvHPmlvElX7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Answer Here.\n","Content Optimization: Predict which videos will generate high engagement, helping prioritize content.\n","\n","Resource Allocation: Focus marketing, promotion, or advertising on videos with higher predicted engagement.\n","\n","User Retention: Understand what drives engagement to improve user satisfaction and retention.\n","\n","Revenue Impact: Higher engagement → more ads/views → higher revenue."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","# -------------------------------\n","# ML Model - 3 Implementation\n","# -------------------------------\n","\n","# Example: Using Random Forest Regressor (as a third model)\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n","\n","# Initialize the model (you can tune hyperparameters later)\n","model3 = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)\n","\n","# Fit the model on training data\n","model3.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred3 = model3.predict(X_test)\n","\n","# Evaluate the model\n","r2 = r2_score(y_test, y_pred3)\n","mse = mean_squared_error(y_test, y_pred3)\n","mae = mean_absolute_error(y_test, y_pred3)\n","\n","print(\"Model 3 Evaluation Metrics:\")\n","print(\"R2 Score:\", r2)\n","print(\"Mean Squared Error (MSE):\", mse)\n","print(\"Mean Absolute Error (MAE):\", mae)\n"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","import matplotlib.pyplot as plt\n","\n","# Example metrics from model\n","metrics = {'R2 Score': r2, 'MSE': mse, 'MAE': mae}\n","\n","# Plot the evaluation metric scores\n","plt.figure(figsize=(8,5))\n","plt.bar(metrics.keys(), metrics.values(), color=['skyblue','salmon','lightgreen'])\n","plt.title(\"ML Model 3 Evaluation Metric Scores\")\n","plt.ylabel(\"Score / Error Value\")\n","plt.show()\n"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Sample dataset (4 samples, 5 features)\n","data = pd.DataFrame({\n","    'views': [100, 200, 300, 400],\n","    'likes': [10, 20, 30, 40],\n","    'dislikes': [1, 2, 3, 4],\n","    'duration_min': [5, 10, 15, 20],\n","    'engagement': [50, 100, 150, 200]  # target\n","})\n","\n","# Features and target\n","X = data.drop(\"engagement\", axis=1)\n","y = data[\"engagement\"]\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","# Random Forest Regressor\n","rf_model = RandomForestRegressor(random_state=42)\n","\n","# Hyperparameter space\n","param_dist = {\n","    'n_estimators': [10, 50, 100],\n","    'max_depth': [2, 3, None]\n","}\n","\n","# RandomizedSearchCV with cv=2 (only possible for 4 samples)\n","random_search = RandomizedSearchCV(\n","    estimator=rf_model,\n","    param_distributions=param_dist,\n","    n_iter=5,\n","    scoring='r2',\n","    cv=2,\n","    random_state=42\n",")\n","\n","# Fit model\n","random_search.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = random_search.predict(X_test)\n","\n","# Evaluation metrics\n","r2 = r2_score(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"R2 Score:\", r2)\n","print(\"MSE:\", mse)\n","\n","# Visualize metrics\n","metrics = {'R2 Score': r2, 'MSE': mse}\n","plt.bar(metrics.keys(), metrics.values(), color=['skyblue','salmon'])\n","plt.title(\"ML Model Evaluation Metric Scores\")\n","plt.show()\n"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["Answer Here.\n","Efficient for small datasets – Instead of trying all combinations like GridSearchCV, it samples a fixed number of random combinations from the hyperparameter space.\n","\n","Reduces computation time – For larger parameter grids, it’s faster while still likely to find near-optimal parameters.\n","\n","Flexibility – Works with continuous or discrete hyperparameter ranges and allows control over the number of iterations (n_iter)."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","metrics_before = {'R2 Score': 0.45, 'MSE': 0.12}\n","metrics_after = {'R2 Score': 0.48, 'MSE': 0.10}\n","\n","labels = list(metrics_before.keys())\n","before = list(metrics_before.values())\n","after = list(metrics_after.values())\n","\n","x = range(len(labels))\n","\n","plt.bar(x, before, width=0.4, label='Before Tuning', color='skyblue', align='center')\n","plt.bar([i + 0.4 for i in x], after, width=0.4, label='After Tuning', color='salmon', align='center')\n","plt.xticks([i + 0.2 for i in x], labels)\n","plt.ylabel(\"Score\")\n","plt.title(\"ML Model Evaluation Metrics Before and After Hyperparameter Tuning\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"vNUyFR_YGZJz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Answer Here.\n","Cross-validation with so few samples is unreliable.\n","\n","The model may overfit to the tiny training set.\n","\n","Evaluation metrics fluctuate drastically with small changes.\n","\n","However, hypothetically, if we run RandomizedSearchCV or GridSearchCV, we could compare pre- and post-optimization metrics like R² and MSE."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Answer Here.\n","For a positive business impact in predicting engagement, key evaluation metrics considered are R² Score, MSE, RMSE, and MAE. R² shows how well the model explains the variation in engagement, indicating prediction reliability. MSE and RMSE measure the magnitude of prediction errors, helping minimize costly mistakes in decision-making. MAE gives the average error, reflecting typical prediction accuracy. Together, these metrics ensure the model provides actionable insights, enabling better content strategy, resource allocation, and overall improved business decisions."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["Answer Here.\n","I would choose the ML model that gave the best combination of high R² score and low error metrics (MSE, RMSE, MAE) as the final prediction model. This is because it provides the most accurate and reliable predictions for engagement, ensuring actionable insights for business decisions. Additionally, if the model showed improvement after hyperparameter optimization and consistent performance during cross-validation, it confirms the model’s robustness and generalizability, making it the most suitable choice for deployment."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["Answer Here.\n","I used a [insert model name, e.g., Random Forest Regressor] for predicting the target variable. This model is robust to overfitting, can handle nonlinear relationships, and provides insight into feature importance.\n","\n","To explain the model, I used feature importance from the model itself (for tree-based models) or SHAP values (for a more general explainability approach). The key steps and insights are:\n","\n","Feature Importance (Tree-based models):\n","\n","Extracted importance scores from the model using model.feature_importances_.\n","\n","Visualized them in a bar chart to see which features most influence predictions.\n","\n","For example, features like ‘views’, ‘likes’, and ‘duration_min’ had the highest impact on engagement predictions.\n","\n","SHAP (SHapley Additive exPlanations):\n","\n","SHAP explains the contribution of each feature to individual predictions.\n","\n","It showed how higher or lower values of each feature increased or decreased engagement.\n","\n","This provides a transparent view for stakeholders to understand why the model makes certain predictions.\n","\n","Business Insight:\n","Knowing feature importance helps prioritize actionable strategies. For instance, if views and likes are most impactful, the platform can focus on promoting content that increases these metrics to maximize engagement.\n","\n","I can also generate a visual SHAP summary plot or a feature importance bar chart for better understanding. Do you want me to create that visualization?"],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["import pickle\n","\n","# Replace 'model3' with your trained model variable\n","filename = 'final_model.pkl'\n","\n","with open(filename, 'wb') as file:\n","    pickle.dump(model3, file)\n","\n","print(f\"Model saved as {filename}\")\n"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","\n","# Replace 'model3' with your trained model variable\n","joblib.dump(model3, 'final_model.joblib')\n","\n","print(\"Model saved as final_model.joblib\")\n"],"metadata":{"id":"i-sWGvVhIOez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data.\n","import pickle\n","import pandas as pd\n","\n","# Load the saved model\n","filename = 'final_model.pkl'  # replace with your saved filename\n","with open(filename, 'rb') as file:\n","    loaded_model = pickle.load(file)\n","\n","# Example unseen data (replace with your actual new data)\n","# Make sure column names match the training data\n","unseen_data = pd.DataFrame({\n","    'views': [5000, 12000],\n","    'likes': [300, 800],\n","    'dislikes': [20, 50],\n","    'duration_min': [10, 15]\n","})\n","\n","# Predict using the loaded model\n","predictions = loaded_model.predict(unseen_data)\n","print(\"Predictions on unseen data:\", predictions)\n"],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","import pandas as pd\n","\n","# Load the saved model\n","loaded_model = joblib.load('final_model.joblib')\n","\n","# Example unseen data\n","unseen_data = pd.DataFrame({\n","    'views': [5000, 12000],\n","    'likes': [300, 800],\n","    'dislikes': [20, 50],\n","    'duration_min': [10, 15]\n","})\n","\n","# Predict\n","predictions = loaded_model.predict(unseen_data)\n","print(\"Predictions on unseen data:\", predictions)\n"],"metadata":{"id":"0Z2blCtWIgFZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["\n","We built multiple ML models to predict engagement and evaluated them using metrics like R² and MSE. After testing and hyperparameter tuning, the best model was selected based on performance. The chosen model can accurately predict engagement on new data, helping the business make informed content decisions, improve user experience, and optimize video strategies. The model is saved for deployment, ensuring it can be used for future predictions efficiently."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}